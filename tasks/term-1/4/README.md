# 4. Java: Word Frequency Counter 
## Особенности реализации
Данная программа реализует интеллектуальную систему выбора механизма обработки текстовых файлов, основанную на анализе размера файла. Архитектура построена вокруг единой точки входа - метода countWords, который автоматически определяет оптимальный способ обработки данных. 
Особенностью является установка практического порога в 55 КБ, что позволяет продемонстрировать работу обоих алгоритмов на тестовых файлах разумного размера. Для файлов меньшего размера применяется метод полной загрузки в память, обеспечивающий максимальную производительность, тогда как для более объемных файлов активируется потоковая обработка, гарантирующая стабильную работу независимо от размера исходных данных.
## Механизм обработки текста и токенизации
В основе системы лежит унифицированный механизм обработки текстовых данных, реализованный в методе processText. Данный подход устраняет дублирование кода и обеспечивает согласованность результатов независимо от выбранного способа обработки. 
Первоначально весь текст приводится к нижнему регистру и исключает дублирование слов из-за различий в регистре написания. Затем выполняется комплексная очистка текста с использованием регулярного выражения [^a-zA-Zа-яА-Я0-9\\s], которое идентифицирует и заменяет на пробелы все символы, не являющиеся буквами английского или русского алфавитов, цифрами или пробельными символами. Финальный этап предполагает разделение обработанной строки на отдельные лексемы с помощью выражения \\s+, что гарантирует корректную обработку последовательностей пробельных символов произвольной длины.
## Специализированные методы обработки
### Метод полной загрузки в память (countWordsInMemory)
Данный подход демонстрирует классическую парадигму обработки данных, когда весь объем информации загружается в оперативную память для последующего анализа. Метод Files.readAllBytes обеспечивает эффективное чтение всего содержимого файла в виде байтового массива, который затем преобразуется в строку для последующей обработки. Такой подход обеспечивает хорошую производительность для файлов небольшого и среднего размера, но имеет ограничение, связанное с объемом доступной оперативной памяти. Ниже представлен пример реализации данного метода:
```java
private Map<String, Integer> countWordsInMemory(Path filePath) {
    try {
        String content = new String(Files.readAllBytes(filePath));
        return processText(content);
            
    } catch (IOException e) {
        throw new RuntimeException("Ошибка чтения файла: " + e.getMessage(), e);
    }
}
```
### Потоковый метод обработки (countWordsStreaming)
Для работы с файлами большого объема реализован потоковый алгоритм. Использование конструкции try-with-resources гарантирует корректное освобождение системных ресурсов, а построчное чтение с помощью BufferedReader позволяет обрабатывать файлы практически неограниченного размера. Ниже представлен пример реализации данного метода:
```java
private Map<String, Integer> countWordsStreaming(Path filePath) {
    Map<String, Integer> frequencies = new HashMap<>();
        
    try (BufferedReader reader = Files.newBufferedReader(filePath)) {
        String line;
            
        while ((line = reader.readLine()) != null) {
             Map<String, Integer> lineFrequencies = processText(line);
            mergeFrequencies(frequencies, lineFrequencies);
        }
            
    } catch (IOException e) {
        throw new RuntimeException("Ошибка чтения файла: " + e.getMessage(), e);
    }
        
    return frequencies;
}
```
Для объединения результатов частичной обработки реализован метод слияния частотных характеристик:
```java
private void mergeFrequencies(Map<String, Integer> target, Map<String, Integer> source) {
    for (Map.Entry<String, Integer> entry : source.entrySet()) {
        target.put(entry.getKey(), target.getOrDefault(entry.getKey(), 0) + entry.getValue());
    }
}
```
Для каждого слова из обрабатываемой строки выполняется обновление соответствующего счетчика, а использование метода getOrDefault обеспечивает обработку как существующих, так и новых слов без необходимости предварительных проверок.
## Вывод частот слов
Первоначально осуществляется сортировка слов по убыванию частоты встречаемости с ограничением вывода двадцатью наиболее распространенными лексемами. Затем вычисляются общее количество слов и количество уникальных. Завершает анализ идентификация наиболее часто встречающегося слова через метод max и ifPresent.
